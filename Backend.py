import fitz
import faiss
from sentence_transformers import SentenceTransformer
import numpy as np
import requests
from groq import Groq

# Load the embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

GROQ_API_TOKEN = "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"

# Initialize the client with the token
client = Groq(api_key= GROQ_API_TOKEN)

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF file.

    Args:
        pdf_path (str): The path to the PDF file.

    Returns:
        str: The extracted text from the PDF.
    """
    text = ""
    with fitz.open(pdf_path) as pdf:
        for page in pdf:
            text += page.get_text()
    return text

def create_embeddings(text, chunk_size=5):
    """
    Creates sentence embeddings from the provided text.

    The text is split into chunks of a specified size, and embeddings
    are generated for each chunk using a pre-trained sentence transformer model.

    Args:
        text (str): The text to be processed.
        chunk_size (int): The number of sentences to include in each chunk.

    Returns:
        tuple: A tuple containing two elements:
            - list: A list of text chunks.
            - np.array: An array of embeddings for the chunks.
    """
    sentences = text.split('. ')
    chunks = []
    
    # Loop through sentences with the specified chunk size
    for i in range(0, len(sentences), chunk_size):
        current_chunk = sentences[i:i + chunk_size]
        chunk_string = ' '.join(current_chunk)
        chunks.append(chunk_string)
        
    embeddings = embedding_model.encode(chunks)
    return chunks, embeddings

def build_faiss_index(embeddings):
    """
    Initializes and stores embeddings in a FAISS index for efficient similarity search.

    Args:
        embeddings (np.array): The array of embeddings to index.

    Returns:
        faiss.Index: A FAISS index containing the embeddings.
    """
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(np.array(embeddings))
    return index

def retrieve_context(query, index, chunks):
    """
    Retrieves the most similar text chunks from the FAISS index based on a query.

    Args:
        query (str): The question or query to find context for.
        index (faiss.Index): The FAISS index containing embeddings.
        chunks (list): The list of text chunks.

    Returns:
        str: A string of context retrieved from the chunks.
    """
    query_embedding = embedding_model.encode([query])
    _, indices = index.search(query_embedding, k=5)
    context = " ".join([chunks[i] for i in indices[0]])
    return context

def query_llama_3(context, question):
    """
    Queries the LLaMA 3 model hosted on Groq Cloud to generate an answer.

    Args:
        context (str): The context from the PDF to inform the answer.
        question (str): The question to be answered.

    Returns:
        str: The generated answer from the LLaMA 3 model.
    """
    prompt = f"Based on the context below, answer the question:\n\nContext: {context}\n\nQuestion: {question}"

    # Initialize completion with streaming enabled
    completion = client.chat.completions.create(
        model="llama3-8b-8192",
        messages=[{"role": "user", "content": prompt}],
        temperature=1,
        max_tokens=1024,
        top_p=1,
        stream=True,
        stop=None,
    )
 
    # Stream and concatenate the response content
    response = ""
    for chunk in completion:
        response += chunk.choices[0].delta.content or ""

    return response

def generate_answer(pdf_path, question):
    """

    Orchestrates the entire process of answering a question based on a PDF file.

    This function combines text extraction, embedding creation, context retrieval,
    and querying the LLaMA 3 model to generate an answer.

    Args:
        pdf_path (str): The path to the PDF file.
        question (str): The question to be answered.

    Returns:
        str: The final answer generated by the LLaMA 3 model based on the PDF content.
    """
    text = extract_text_from_pdf(pdf_path)
    chunks, embeddings = create_embeddings(text)
    index = build_faiss_index(np.array(embeddings))
    context = retrieve_context(question, index, chunks)
    answer = query_llama_3(context, question)
    return answer
